\documentclass[Journal,SectionNumbers,SingleSpace,InsideFigs]{ascelike}

%\usepackage{subfigure}
%\usepackage{epsfig}
%\usepackage{timesmt}

\usepackage[utf8]{inputenc}
\usepackage[OT1]{fontenc}
\usepackage{graphicx}
\usepackage[english]{babel}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}

\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{url}
\usepackage{tikz}
\usetikzlibrary{shapes.misc,fit}

%\usepackage[bookmarks]{hyperref}
\usepackage[colorlinks=true,citecolor=red,linkcolor=black]{hyperref}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}
\newcommand{\posrealszero}{\reals_{\ge 0}}
\newcommand{\naturals}{\mathbb{N}}

\newcommand{\dd}{\,\mathrm{d}}

\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\renewcommand{\vec}[1]{{\bm#1}}

\newcommand{\uz}{^{(0)}} % upper zero
\newcommand{\un}{^{(n)}} % upper n
\newcommand{\ui}{^{(i)}} % upper i

\newcommand{\ul}[1]{\underline{#1}}
\newcommand{\ol}[1]{\overline{#1}}

\newcommand{\Rsys}{R_\text{sys}}
\newcommand{\lRsys}{\ul{R}_\text{sys}}
\newcommand{\uRsys}{\ol{R}_\text{sys}}

\newcommand{\Fsys}{F_\text{sys}}
\newcommand{\lFsys}{\ul{F}_\text{sys}}
\newcommand{\uFsys}{\ol{F}_\text{sys}}

\def\Rsys{R_\text{sys}}
\def\Tsys{T_\text{sys}}

\newcommand{\E}{\operatorname{E}}
\newcommand{\V}{\operatorname{Var}}
\newcommand{\wei}{\operatorname{Wei}} % Weibull Distribution
\newcommand{\ig}{\operatorname{IG}}   % Inverse Gamma Distribution

\def\yz{y\uz}
\def\yn{y\un}
%\def\yi{y\ui}
\newcommand{\yfun}[1]{y^{({#1})}}
\newcommand{\yfunl}[1]{\ul{y}^{({#1})}}
\newcommand{\yfunu}[1]{\ol{y}^{({#1})}}

\def\ykz{y\uz_k}
\def\ykn{y\un_k}

\def\yzl{\ul{y}\uz}
\def\yzu{\ol{y}\uz}
\def\ynl{\ul{y}\un}
\def\ynu{\ol{y}\un}
\def\yil{\ul{y}\ui}
\def\yiu{\ol{y}\ui}

\def\ykzl{\ul{y}\uz_k}
\def\ykzu{\ol{y}\uz_k}
\def\yknl{\ul{y}\un_k}
\def\yknu{\ol{y}\un_k}


\def\nz{n\uz}
\def\nn{n\un}
%\def\ni{n\ui}
\newcommand{\nfun}[1]{n^{({#1})}}
\newcommand{\nfunl}[1]{\ul{n}^{({#1})}}
\newcommand{\nfunu}[1]{\ol{n}^{({#1})}}

\def\nkz{n\uz_k}
\def\nkn{n\un_k}
\newcommand{\nkzfun}[1]{n\uz_{#1}}


\def\nzl{\ul{n}\uz}
\def\nzu{\ol{n}\uz}
\def\nnl{\ul{n}\un}
\def\nnu{\ol{n}\un}
\def\nil{\ul{n}\ui}
\def\niu{\ol{n}\ui}

\def\nkzl{\ul{n}\uz_k}
\def\nkzu{\ol{n}\uz_k}
\def\nknl{\ul{n}\un_k}
\def\nknu{\ol{n}\un_k}


\def\taut{\tau(\vec{t})}
\def\ttau{\tilde{\tau}}
\def\ttaut{\ttau(\vec{t})}

\def\MZ{\mathcal{M}\uz}
\def\MN{\mathcal{M}\un}

\def\MkZ{\mathcal{M}\uz_k}
\def\MkN{\mathcal{M}\un_k}

\def\PkZ{\Pi\uz_k}
\def\PkN{\Pi\un_k}
\newcommand{\PZi}[1]{\Pi\uz_{#1}}


\def\tnow{t_\text{now}}
\def\tpnow{t^+_\text{now}}

\newcommand{\comments}[1]{{\small\color{gray} #1}}

\newtheorem{example}{Example}

\allowdisplaybreaks

\title{Notes for ISIPTA poster}
\author{Gero Walter, Frank P.A. Coolen, Simme Douwe Flapper}

\begin{document}
\title{ROBUST BAYESIAN RELIABILITY FOR COMPLEX SYSTEMS\\ or \ldots}

\author{
Gero Walter%
\thanks{
School of Industrial Engineering,
Eindhoven University of Technology, Eindhoven, The Netherlands.
E-mail: g.walter@tue.nl.},
\ Ph.D.
\\
and
Frank P.A. Coolen%
\thanks{
Department of Mathematical Sciences,
Durham University, Durham, United Kingdom.
E-mail: frank.coolen@durham.ac.uk.},
\ Ph.D.
\\
and ???
}

\maketitle

Invited for special issue ``The treatment of uncertainty in risk and reliability modelling and decision making''
(special issue code SI022A), guest editors Luca Podolfillini, Bruno Sudret, Enrico Zio,
in ASCE-ASME Journal of Risk and Uncertainty in Engineering Systems: Part A (Civil Engineering)

***``For most ASCE journals, the maximum number of words and word-equivalents is 10,000 for technical papers''


% slightly adapted ESREL abstract
\begin{abstract}
In reliability engineering, data about failure events is often scarce.
To arrive at meaningful estimates for the reliability of a system,
it is therefore often necessary to also include expert information in the analysis,
which is straightforward in the Bayesian approach by using an informative prior distribution.
%
A problem that then can arise is called prior-data conflict:
from the viewpoint of the prior, the observed data seem very surprising,
i.e., the information from data is in conflict with the prior assumptions.
It has been recognised that models based on conjugate priors can be insensitive to prior-data conflict,
in the sense that the spread of the posterior distribution does not increase in case of such a conflict,
thus conveying a false sense of certainty by communicating that we can quantify the reliability of a system quite precisely when in fact we cannot.
%
We present an approach to mitigate this issue, by considering sets of prior distributions
to model vague knowledge on component lifetimes,
and study how surprisingly early or late component failures
affect the prediction of the reliability of a system with arbitrary layout,
making use of survival signature to characterise the system under study.
Our approach can be seen as a robust Bayesian procedure or imprecise probability method
that appropriately reflects surprising data in the posterior system survival function or other posterior inferences.
\end{abstract}

\KeyWords{System Reliability, Imprecise Probability, Survival Signature, Robust Bayesian, Remaining Useful Life}

***slightly adapted ESREL abstract, must be 150--175 words long (this has 226), may not contain references or mathematics.


\section{Introduction}

In reliability engineering, a central task is to describe the reliability of a complex system.
This is usually done by determining the \emph{reliability function} $R(t)$,
in other contexts also known as the \emph{survival function} $S(t)$,
giving the probability that the system has not failed by time $t$:
\begin{align}
\Rsys(t) = P(\Tsys \geq t)\,,
\end{align}
where $\Tsys$ is the random variable giving the failure time of the system. %
%\footnote{}
Based on the distribution of $\Tsys$, which can also be expressed
in terms of the cumulative distribution function $F_\text{sys}(t) = 1 - \Rsys(t)$,
%the density $p_\text{sys}(t)$ or the hazard rate $\lambda_\text{sys}(t)$,
decisions about, e.g., scheduling of maintenance work can be made.

Often, there is no failure data for the system itself
(e.g., if the system is a prototype, or the system is used under unique circumstances),
but there is some information about failure times for the components the system is made of.
We are able to analyse systems of arbitrary system structures, %layout of components,
i.e., any combination and nesting of series, parallel, k-out-of-n, or bridge-type arrangements,
by use of the survival signature \cite{2012:survsign}.
In this paper, we assume that components can be divided into $K$ different groups,
and components within each group $k$ ($k=1, \ldots, K$) can be assumed to be exchangeable,
i.e., to follow the same failure time distribution.
We denote components of group $k$ as \emph{type $k$ components},
and assume independence between component types,
such that failure times of components of different types are independent.
We assume that type $k$ component lifetimes $T_i^k$ ($i = 1, \ldots, n_k$)
are Weibull distributed, with type-specific parameters $\beta_k$ (shape) and $\lambda_k$ (scale).
***type-specific shape not implemented yet!***

Focusing on the prototype application,
we assume that our observations consist solely of the failure times of components in this system up to time $\tnow$,
such that the failure times of components that have not failed by $\tnow$ are right-censored,
and calculate $\Rsys(t \mid t > \tnow) = P(\Tsys \geq t \mid t > \tnow)$,
which can be used to determine the remaining useful life of the system
(in short RUL, see, e.g., \citeNP{2014:rul-review}).
%This sparse information is complemented by expert information on component lifetimes $T_i^k$.

The Bayesian approach allows to base estimation of the component failure distributions
on both data and further knowledge not given by the data,
the latter usually provided in the form of expert knowledge.
This knowledge is encoded in form of a so-called prior distribution,
a distribution on the parameters of the component lifetime distributions.
Expert knowledge is especially important when there is very few data on the components (like in our scenario),
as only with its help meaningful estimates for the system reliability can be made.

However, the choice for the prior distribution to encode the given expert knowledge is often debatable,
and a specific choice of prior is difficult to justify.
A way to deal with this is to employ sensitivity analysis,
i.e., studying the effect of different choices of prior distribution on the quantities of interest
(in our case, the system reliability function, which, in Bayesian terms, is a predictive distribution).
This idea has been explored in systematic sensitivity analysis, or robust Bayesian methods
(for an overview on this approach, see, e.g.,
\citeNP{1994:berger} or \citeNP{2000:rios}). %\citeNP{2005:ruggeri}, \citeNP{2000:bergerinsuaruggeri}

The work we present here can be seen as belonging to the robust Bayesian approach,
as our work uses sets of priors. However, our focus and interpretation is slightly different,
as we consider the result of our procedure, sets of reliability functions, as the proper result,
while a robust Bayesian would base his analyses on a single reliability function from the set
in case (s)he was able to conclude that quantities of interest are not `too sensitive' to the choice of prior.
In contrast, our viewpoint is rooted in the theory of imprecise or interval probability \cite{1991:walley,itip},
where sets of distributions are used to express the precision of probability statements themselves:
the smaller the set, the more precise the probability statement.
Indeed, the system reliability function $\Rsys(t)$ is a collection of probability statements,
and a small set for $\Rsys(t)$ will indicate that we can quantify the reliability of the system quite precisely,
while a large set will indicate that our knowledge about $\Tsys$ is rather shaky.

In line with imprecise or interval probability methods, we will thus have, for each $t$,
a lower reliability $\lRsys(t) = \ul{P}(T_\text{sys} \geq t)$,
and an upper reliability $\uRsys(t) = \ol{P}(T_\text{sys} \geq t)$.
We will explain in Sections~\ref{sec:modforsurpr} and *** how these bounds are obtained
based on a set of prior distributions on the scale parameter of the component lifetime distribution.

The central merit of our method is that it adequately reflects prior-data conflict
(see, e.g., \citeNP{2006:evans}),
i.e.\ the conflict that can arise between prior assumptions on component lifetimes
and observed behaviour of components in the system under study.
As we will show in Section~\ref{sec:weibull}, when taking the standard choice of a conjugate prior,
prior-data conflict is ignored, as the spread of the posterior distribution does not increase in case of such a conflict,
ultimately conveying a false sense of certainty
by communicating that we can quantify the reliability of a system quite precisely when in fact we can not.
%
In contrast, our method will indicate prior-data conflict by wider bounds for $\Rsys(t)$.
This behaviour is obtained by a specific choice for the set of priors (see \citeNP{Walter2009a} and \citeNP{diss} \S 3.1.4)
which leads to larger sets of posterior distributions when prior knowledge and data are in conflict
(see Section~\ref{sec:modforsurpr} for more details).

Due to the iterative nature of the Bayesian framework,
it is possible to use a set of posteriors based on component test data as the set of priors
instead of a purely expert-based set of priors as discussed so far.
In that case, prior-data conflict sensitivity allows to uncover also a conflict between
current observations in the running system and past observations from component tests.

This work extends \ldots ***

***comment on shape parameter $\beta_k$ being fixed***

We study the effect of surprisingly early or late component failures,
%on the system reliability prediction,
showing that observations in conflict to prior assumptions
indeed lead to more cautious system reliability predictions.

  
The paper is organized as follows.
In Section~\ref{sec:weibull}, we describe a Bayesian analysis for Weibull component lifetimes
and illustrate the issue of prior-data conflict.
Section~\ref{sec:modforsurpr} then details the use of sets of priors
for the scale parameter $\lambda_k$ of the Weibull distribution,
showing how this mitigates the prior-data conflict issue for $\lambda_k$.
***

%In the last section, we will briefly describe
%how the approach can be generalized to arbitrary system structures
%using the survival signature \cite{2012:survsign},
%which allows for different types of components and
%the inclusion of data from component tests, as in \citeNP{2014:bayessurvsign}.
%Information on the distribution of component failure times $T_i$,
%where $i = 1, \ldots, n$ if the system consists of $n$ components,
%can then be used to derive the distribution of $T_\text{sys}$.


\section{Bayesian Analysis of Weibull Lifetimes}
\label{sec:weibull}

We consider a system with components of $k=1,\ldots,K$ different types;
for each type $k$, there are $n_k$ exchangeable components in the system.
For each type $k$ component, we assume for its lifetime $T_i^k$ ($i=1,\ldots,n_k$, $k = 1, \ldots, K$)
a Weibull distribution with fixed shape parameter $\beta_k > 0$,
in short $T_i^k \mid \lambda_k \sim \wei(\beta_k,\lambda_k)$,
with density and cdf%
\footnote{Our approach would be possible also for other parametric lifetime distributions
that form a canonical exponential family
(see, e.g., \citeNP[p.~202 and 272f]{2000:bernardosmith}, or \citeNP[p.~8]{diss}).}
\begin{linenomath*}
\begin{align}
f_k(t_i^k \mid \lambda_k) &= \frac{\beta_k}{\lambda_k} (t_i^k)^{\beta_k-1} e^{-\frac{(t_i^k)^{\beta_k-1}}{\lambda_k}}\,, \\
F_k(t_i^k \mid \lambda_k) &= 1 - e^{-\frac{(t_i^k)^{\beta_k}}{\lambda_k}} = P(T_i^k \leq t_i^k \mid \lambda_k)\,,
\end{align}
\end{linenomath*}
where $\lambda_k > 0$ and $t > 0$.%
\footnote{The shape parameter $\beta_k$ determines whether the hazard rate is increasing ($\beta_k > 1$)
or decreasing ($\beta_k < 1$) over time.
For $\beta_k=1$, we obtain the Exponential distribution with constant hazard rate as a special case.
The value for $\beta_k$ will thus often be clear from practical considerations.}
The scale parameter $\lambda_k$ can be interpreted through the relation
\begin{linenomath*}
\begin{align}
\E[T_i^k \mid \lambda_k] &= \lambda_k^{1/\beta_k}\, \Gamma(1 + 1/\beta_k)\,.
\label{eq:lambdainterpret}
\end{align}
\end{linenomath*}
For encoding expert knowledge about the reliability of the components,
we need to assign a prior distribution over the scale parameter $\lambda_k$.
A convenient choice is to use the inverse Gamma distribution,
commonly parametrized in terms of the hyperparameters $a_k > 0$ and $b_k > 0$:
%Conjugate prior is $\lambda_k \sim \ig(a_k,b_k)$:
\begin{linenomath*}
\begin{align}
f_{\lambda_k}(\lambda_k\mid a_k,b_k) &= \frac{(b_k)^{a_k}}{\Gamma(a_k)} \lambda_k^{-a_k -1} e^{-\frac{b_k}{\lambda_k}}
\end{align}
\end{linenomath*}
in short, $\lambda_k \mid a_k, b_k \sim \ig(a_k,b_k)$.
The inverse Gamma is convenient because it is a conjugate prior,
i.e., the posterior obtained by Bayes' rule is again inverse Gamma and thus easily tractable;
the prior parameters only need to be updated to obtain the posterior parameters.

In the standard Bayesian approach, 
one has to fix a prior by choosing values for $a_k$ and $b_k$
to encode specific prior information about component lifetimes.
In our imprecise approach, we allow instead these parameters
to vary in a set, this is advantageous also
because expert knowledge is often vague,
and it is difficult for the expert(s) to pin down precise hyperparameter values.
For the definition of the hyperparameter set,
we use however a parametrization in terms of $\nz > 1$ and $\yz > 0$ instead of $a_k$ and $b_k$,
where we drop the index $k$ for the discussion about the prior model in the following,
keeping in mind that each component type will have its own specific parameters.
We use
%\begin{linenomath*}
%\begin{align}
$\nz = a - 1$ and
%&
$\yz = b / \nz$,
%\end{align}
%\end{linenomath*}
where $\yz$ can be interpreted as the prior guess for the scale parameter $\lambda$,
as $\E[\lambda\mid\nz,\yz] = \yz$.
This parametrization also makes the nature of the combination
of prior information and data through Bayes' rule more clear:
After observing $n$ component lifetimes $\vec{t} = (t_1, \ldots, t_n)$,
the updated parameters are
\begin{linenomath*}
\begin{align}
\nn &= \nz + n\,, 
&
\yn &=  \frac{\nz \yz + \taut}{\nz + n}\,,
\label{eq:ig-update}
\end{align}
\end{linenomath*}
where $\taut = \sum_{i=1}^n (t_i)^\beta$. %
%\footnote{
We thus have
\begin{linenomath*}
\begin{align}
\lambda \mid \nz, \yz, \vec{t} \sim \ig(\nz + n + 1, \nz \yz + \taut). %}
\label{eq:ig-update-alpha}
\end{align}
\end{linenomath*}
%In terms of canonical parameters $\nkz$ and $\ykz$, we assume $\lambda_k \mid \nkz,\ykz \sim \ig(\nkz + 1, \nkz\ykz)$.
From the simple update rule \eqref{eq:ig-update}, we see that
$\yn$ is a weighted average of the prior parameter $\yz$ and the maximum likelihood (ML) estimator $\taut/n$,
with weights $\nz$ and $n$, respectively.
$\nz$ can thus be interpreted as a prior strength or pseudocount,
indicating how much our prior guess should weigh against the $n$ observations.
Furthermore, $\V[\lambda\mid\nz,\yz] = (\yz)^2 / (1 - 1/\nz)$,
so for fixed $\yz$, the higher $\nz$,
the more probability mass is concentrated around $\yz$. %$p(\lambda\mid\nz,\yz)$ 

However, the weighted average structure for $\yn$
is behind the problematic behaviour in case of prior-data conflict.
Assume that from expert knowledge we expect
to have a mean component lifetime of 9 weeks.
Using \eqref{eq:lambdainterpret}, with $\beta=2$ we obtain $\yz = 103.13$.
We choose $\nz = 2$, so our prior guess for the mean component lifetime
counts like having two observations with this mean.
If we now have a sample of two observations
with surprisingly early failure times $t_1 = 1$ and $t_2 = 2$,
using \eqref{eq:ig-update} we get $\nfun{2} = 4$
and $\yfun{2} = \frac{1}{4}(2 \cdot 103.13 + 1^2 + 2^2) = 52.82$,
so our posterior expectation for the scale parameter $\lambda$ is $52.82$,
equivalent to a mean component lifetime of $6.44$ weeks.
The posterior standard deviation (sd) for $\lambda$ is $60.99$.
Compared to the prior standard deviation of $145.85$,
the posterior expresses now more confidence that mean lifetimes are around $\yfun{2} = 52.82$
than the prior had about $\yz = 103.13$.
This irritating conclusion is illustrated in Figure~\ref{fig:weibull-pdc};
the posterior cdf is shifted halfway towards the values for $\lambda$
that the two observations suggest
(the ML estimator for $\lambda$ would be $2.5$),
and is steeper than the prior (so the pdf is more pointed),
thus conveying a false sense of certainty about $\lambda$.%
\footnote{This is a general problem in Bayesian analysis with canonical conjugate priors.
For such priors, the same update formula \eqref{eq:ig-update} applies,
and so conflict is averaged out, for details see \citeN{Walter2009a} and \citeN[\S 3.1.4 and \S A.1.2]{diss}.}
We would obtain almost the same %values of $\nfun{2}$ and $\yfun{2}$, and so the same 
posterior distribution
if we had assumed the mean component lifetime to be 7 weeks (so $\yz = 62.39$),
and observed lifetimes $t_1 = 6$, $t_2 = 7$ in line with our expectations.
It seems unreasonable to make the same probability statements on component lifetimes in these two fundamentally different scenarios.
%
%\iffalse
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{fig1}
\caption{Prior and posterior cdf for $\lambda$ given surprising observations;
the conflict between prior assumptions and data is averaged out,
with a more pointed posterior giving a false sense of certainty.}
\label{fig:weibull-pdc}
\end{figure}
%\fi


\section{Models reflecting surprising data / Sets of Priors}
\label{sec:modforsurpr}

Set of priors $\MkZ$ defined by $(\nkz,\ykz) \in \PkZ = [\nkzl,\nkzu] \times [\ykzl,\ykzu]$.

***illustrate prior-data conflict similar to ESREL paper


\section{Robust RUL Estimation for Complex Systems via the Survival Signature}

\subsection{Right-censored Lifetimes}

Observing a one of a kind system with $n_k$ components of type $k$ running until $\tnow$,
where $e_k$ components have failed by $\tnow$, and $n_k - e_k$ components still function:
\begin{linenomath*}
\begin{align}
\mbf{t}^k_{e_k;n_k} &= \big( \underbrace{t^k_1, \ldots, t^k_{e_k}}_{e_k \text{failure times}},
                             \underbrace{\tpnow, \ldots, \tpnow}_{n_k-e_k \text{censored obs.}} \big)
\end{align}
\end{linenomath*}

***change to general right-censoring? So $n_k-e_k \to c_k$, $e_k+c_k=n_k$,
introduce sets of observed and censored?

\subsection{Bayesian Estimation of Component Scale Parameter}

Posterior:
\begin{linenomath*}
\begin{align}
f_{\lambda_k\mid\ldots}(\lambda_k\mid\nkz,\ykz,\mbf{t}^k_{e_k;n_k})
 &\propto f_{\lambda_k}(\lambda_k)
          \big[ 1- F_k(\tnow\mid\lambda_k) \big]^{n_k-e_k}
          \prod_{i=1}^{e_k} f_k(t_i^k \mid \lambda_k) 
\end{align}
\end{linenomath*}
and so $\lambda_k\mid\nkz,\ykz,\mbf{t}^k_{e_k;n_k} \sim \ig(\nkn + 1, \nkn\ykn)$, where
\begin{linenomath*}
\begin{align}
\nkn + 1 &= \nkz + e_k + 1 \\
\nkn\ykn &= \nkz\ykz + (n_k-e_k) \tnow^\kappa + \sum_{i=1}^{e_k} (t_i^k)^\kappa
\end{align}
\end{linenomath*}
so the posterior expectation for $\lambda$ is
\begin{linenomath*}
\begin{align}
\E[\lambda_k\mid\nkn,\ykn] &= \ykn
                            = \frac{\nkz}{\nkz + e_k} \ykz +
                              \frac{e_k}{\nkz + e_k} \cdot \Big[\frac{n_k-e_k}{e_k} \tnow^\kappa + \frac{1}{e_k}\sum_{i=1}^{e_k} (t_i^k)^\kappa \Big]
\end{align}
\end{linenomath*}

\subsection{RUL using the Survival Signature}

System survival calculated by %(see Risk Analysis paper eq. (7))
\begin{linenomath*}
\begin{align}
P(\Tsys > t) &= \sum_{l_1=0}^{n_1} \cdots \sum_{l_K=0}^{n_K} \Phi(l_1,\ldots,l_K) P\Big( \bigcap_{k=1}^K \{ C^k_t = l_k\} \Big) \\
\intertext{ and assuming components of different types are independent,}
             &= \sum_{l_1=0}^{n_1} \cdots \sum_{l_K=0}^{n_K} \Phi(l_1,\ldots,l_K) \prod_{k=1}^K P(C^k_t = l_k)
\end{align}
\end{linenomath*}
where the survival signature $\Phi(l_1,\ldots,l_K)$ is non-decreasing in each $l_k$ for coherent systems,
and $P(C^k_t = l_k)$ is the (predictive) probability that exactly $l_k$ components of type $k$ function at time $t$.

A priori, we have, for $l_k = 0,1,\ldots, n_k$,
\begin{linenomath*}
\begin{align}
P(C^k_t = l_k\mid\nkz,\ykz)
 &= { n_k \choose l_k} \int [F_k(t \mid\lambda_k)]^{n_k-l_k}
                        [1 - F_k(t \mid\lambda_k)]^{l_k} f_{\lambda_k}(\lambda_k\mid\nkz,\ykz) \dd \lambda_k
\end{align}
\end{linenomath*}
under the assumption that components of the same type are independent given $\lambda_k$.

\subsection{Posterior Predictive Distribution}

For the situation with the system observed until $\tnow$, with data $\mbf{t}^k_{e_k;n_k}$,
we need the posterior predictive probability $P(C^k_t = l_k\mid\nkz,\ykz, \mbf{t}^k_{e_k;n_k})$.

For $l_k > n_k - e_k$, we must have $P(C^k_t = l_k\mid\nkz,\ykz, \mbf{t}^k_{e_k;n_k}) = 0$,
as at most $n_k - e_k$ components can function for $t > \tnow$.

%(Should one work with the subsystem consisting only of the non-failed $n_k - e_k$ components?)\\

A posteriori, we have, for $l_k = 0,1,\ldots, n_k - e_k$,
\begin{linenomath*}
\begin{align}
P(C^k_t = l_k\mid\nkz,\ykz, \mbf{t}^k_{e_k;n_k})
 &= { n_k - e_k \choose l_k} \int [P_k(T \leq t \mid T > \tnow, \lambda_k)]^{n_k - e_k - l_k} \times \\ & \hspace*{2ex}
                              [1 - P_k(T \leq t \mid T > \tnow, \lambda_k)]^{l_k}
    f_{\lambda_k\mid\ldots}(\lambda_k\mid\nkz,\ykz,\mbf{t}^k_{e_k;n_k}) \dd \lambda_k
\end{align}
\end{linenomath*}

Now,
\begin{linenomath*}
\begin{align}
P_k(T \leq t \mid T > \tnow, \lambda_k)
 &= \frac{P_k(\tnow < T \leq t \mid\lambda_k)}{P_k(T > \tnow \mid \lambda_k)}
  = \frac{F_k(t\mid\lambda_k) - F_k(\tnow\mid\lambda_k)}{1-F_k(\tnow\mid\lambda_k)} \\
 &= \frac{e^{-\frac{(\tnow)^\kappa}{\lambda_k}} - e^{-\frac{t^\kappa}{\lambda_k}}}{e^{-\frac{(\tnow)^\kappa}{\lambda_k}}}
  = 1 - e^{-\frac{t^\kappa - (\tnow)^\kappa}{\lambda_k}}
\end{align}
\end{linenomath*}

With this and the posterior substituted in, we get
\begin{linenomath*}
\begin{align}
\lefteqn{P(C^k_t = l_k\mid\nkz,\ykz, \mbf{t}^k_{e_k;n_k})} \\
 &= { n_k - e_k \choose l_k} \int \Big[1 - e^{-\frac{t^\kappa - (\tnow)^\kappa}{\lambda_k}}\Big]^{n_k - e_k - l_k}
                                  \Big[    e^{-\frac{t^\kappa - (\tnow)^\kappa}{\lambda_k}}\Big]^{l_k} \times \\ & \hspace*{28ex}
    \frac{[\nkn\ykn]^{\nkn + 1}}{\Gamma(\nkn+1)} \lambda_k^{-(\nkn + 1) - 1} e^{-\frac{\nkn\ykn}{\lambda_k}} \dd \lambda_k \\
 &= { n_k - e_k \choose l_k} \sum_{j=0}^{n_k-e_k-l_k} (-1)^j { n_k - e_k - l_k \choose j} \frac{[\nkn\ykn]^{\nkn + 1}}{\Gamma(\nkn+1)} 
    \times \\ & \hspace*{25ex}
    \int \lambda_k^{-(\nkn + 1) - 1} e^{-\frac{(l_k + j) (t^\kappa - (\tnow)^\kappa) + \nkn\ykn}{\lambda_k}} \dd \lambda_k
\end{align}
\end{linenomath*}

The terms remaining under the integral form the core of an $\ig(\nkn + 1, \nkn\ykn + (l_k + j) (t^\kappa - (\tnow)^\kappa))$,
allowing us to solve the integral using the corresponding normalization constant.
\begin{linenomath*}
\begin{align}
\lefteqn{P(C^k_t = l_k\mid\nkz,\ykz, \mbf{t}^k_{e_k;n_k})} \\
 &= { n_k - e_k \choose l_k} \sum_{j=0}^{n_k-e_k-l_k} (-1)^j { n_k - e_k - l_k \choose j}
    \left(\frac{\nkn\ykn}{\nkn\ykn + (l_k + j) (t^\kappa - (\tnow)^\kappa)}\right)^{\nkn + 1} \\
 &= \sum_{j=0}^{n_k-e_k-l_k} (-1)^j \frac{(n_k - e_k)!}{l_k! j! (n_k - e_k - l_k - j)!}   
    \left(\frac{\nkn\ykn}{\nkn\ykn + (l_k + j) (t^\kappa - (\tnow)^\kappa)}\right)^{\nkn + 1} \\
 &= \sum_{j=0}^{n_k-e_k-l_k} (-1)^j \frac{(n_k - e_k)!}{l_k! j! (n_k - e_k - l_k - j)!} \times \\ & \hspace*{13ex}  
    \left(\frac{\nkz\ykz + \sum_{i=1}^{e_k} (t_i^k)^\kappa + (n_k-e_k) (\tnow)^\kappa }%
               {\nkz\ykz + \sum_{i=1}^{e_k} (t_i^k)^\kappa + (n_k-e_k-l_k-j) (\tnow)^\kappa + (l_k + j) t^\kappa }\right)^{%
    \nkz + e_k + 1} 
\end{align}
\end{linenomath*}
for $l_k \in \{0,1,\ldots,n_k-e_k\}$.

We actually need to compute only those $P(C^k_t = l_k\mid \ldots)$ with $l_k$'s for which $\Phi(l_1,\ldots,l_K) > 0$.

Seen as function in $t$, we see that $t$ appears $n_k-e_k-l_k+1$ times,
once in each summand, in the denominator of the fraction powered to the $\nkn+1$,
and summands whith even $j$ are decreasing in $t$,
while summands whith odd $j$ are increasing in $t$.
Nevertheless, in total $P(C^k_t = l_k\mid\nkz,\ykz, \mbf{t}^k_{e_k;n_k})$
must be decreasing in $t$, as the remaining $n_k-e_k$ components continue to age,
and so the probability that $l_k$ of them remain operational must decrease.

\subsection{Optimizing over Sets of Parameters}

%I would think that
%$\min / \max_{(\nkz,\ykz) \in \PkZ} P(C^k_t = l_k\mid\nkz,\ykz, \mbf{t}^k_{e_k;n_k})$
%must be solved by numeric optimization for each $k, l_k, t$ 
%because of the alternating signs for the summands.
%(In absolute terms, each summand is increasing in $\ykz$, but not monotone in $\nkz$.)

%(Box-constraint optimization possible, e.g., with option \texttt{L-BFGS-B} of \texttt{optim} in \textbf{R}.)
Similar to the considerations about $t$, as higher values for $\ykz$ mean higher expected lifetimes for the components,
the larger $\ykz$, the higher the probability that many components survive
($P(C^k_t = l_k\mid\ldots)$ high for large $l_k$)
and, with that, the lower the probability that few or no components survive
($P(C^k_t = l_k\mid\ldots)$ low for small $l_k$).

In terms of the cumulative probability mass function (cmf) for $C^k_t$, which can be written as 
\begin{linenomath*}
\begin{align}
%F_{C^k_t}(l_k \mid \nkz,\ykz, \mbf{t}^k_{e_k;n_k}) = P(C^k_t \leq l_k\mid\nkz,\ykz, \mbf{t}^k_{e_k;n_k}) 
% = \sum_{q=0}^{l_k} P(C^k_t = q\mid\nkz,\ykz, \mbf{t}^k_{e_k;n_k})
F_{C^k_t}(l_k \mid \ldots) = P(C^k_t \leq l_k\mid\ldots) = \sum_{q=0}^{l_k} P(C^k_t = q\mid\ldots)\,,
\end{align}
\end{linenomath*}
we therefore conclude that
the lower cmf $\ul{F}(C^k_t = l_k\mid \ldots)$ must be obtained with the lower bounds for all $\ykz$, $k=1,\ldots,K$,
and the upper cmf $\ol{F}(C^k_t = l_k\mid \ldots)$ must be obtained with the respective upper bounds.

***proove rigorously (would need a kind of likelihood difference order)?
%Do I get your comments right that based on lower and upper bounds
%$\ul{P}(C^k_t = l_k\mid \ldots)$ and $\ol{P}(C^k_t = l_k\mid \ldots)$ for all $l_k = 0, 1, \ldots, n_k - e_k$,
%we can easily get lower and upper bounds for the cdf $F_{C^k_t}(l_k \mid \ldots)$?

Furthermore, higher expected lifetimes for the components must mean
a longer system survival, so in turn the $\ul{F}_{C^k_t}(l_k \mid \ldots)$'s
should give us the lower bound for $P(\Tsys > t\mid\ldots)$,
and the $\ol{F}_{C^k_t}(l_k \mid \ldots)$'s the upper bound.
Note that e.g.\ $\ul{F}_{C^k_t}(l_k \mid \ldots)$ may, however, not correspond to a single parameter pair $(\nkz, \ykz)$.
Also, in the system survival equation ($t > \tnow$)
\begin{linenomath*}
\begin{align} \label{eq:25}
\lefteqn{%
P(\Tsys > t\mid\{\nkz,\ykz, \mbf{t}^k_{e_k;n_k}\}_{k=1}^K)} \hspace*{10ex}\nonumber\\ 
 &= \sum_{l_1=0}^{n_1-e_1} \cdots \sum_{l_K=0}^{n_K-e_K} \Phi(l_1,\ldots,l_K) \prod_{k=1}^K
    P(C^k_t = l_k\mid\nkz,\ykz, \mbf{t}^k_{e_k;n_k})
\end{align}
\end{linenomath*}
we cannot simply plug in all the $\ul{P}(C^k_t = l_k\mid \ldots)$ %and $\ol{P}(C^k_t = l_k\mid \ldots)$
to get $\ul{P}(\Tsys > t\mid\nkz,\ykz, \mbf{t}^k_{e_k;n_k})$, % and $\ol{P}(\Tsys > t\mid\nkz,\ykz, \mbf{t}^k_{e_k;n_k})$,
as the $\ul{P}(C^k_t = l_k\mid \ldots)$ could correspond to different $(\nkz,\ykz)$ for each $l_k$.
(It would give us a lower bound for $\ul{P}(\Tsys > t\mid\ldots)$, however, but it might be very coarse.)

Writing out Equation~\eqref{eq:25}, we get
\begin{linenomath*}
\begin{align}
\lefteqn{P(\Tsys > t\mid\{\nkz,\ykz, \mbf{t}^k_{e_k;n_k}\}_{k=1}^K)} \\
 &= \sum_{l_1=0}^{n_1-e_1} \cdots \sum_{l_K=0}^{n_K-e_K} \Phi(l_1,\ldots,l_K) \prod_{k=1}^K
    P(C^k_t = l_k\mid\nkz,\ykz, \mbf{t}^k_{e_k;n_k}) \\
 &= \sum_{l_1=0}^{n_1-e_1} \cdots \sum_{l_K=0}^{n_K-e_K} \Phi(l_1,\ldots,l_K) \prod_{k=1}^K
    \sum_{j=0}^{n_k-e_k-l_k} (-1)^j \frac{(n_k - e_k)!}{l_k! j! (n_k - e_k - l_k - j)!} \times \\ & \hspace*{45ex}
    \left(\frac{\nkn\ykn}{\nkn\ykn + (l_k + j) (t^\kappa - (\tnow)^\kappa)}\right)^{\nkn + 1} \\
 &= \sum_{l_1=0}^{n_1-e_1} \cdots \sum_{l_K=0}^{n_K-e_K} \Phi(l_1,\ldots,l_K) \prod_{k=1}^K \label{eq:30}
    \sum_{j=0}^{n_k-e_k-l_k} (-1)^j \frac{(n_k - e_k)!}{l_k! j! (n_k - e_k - l_k - j)!} \times \\ & \hspace*{17ex}
    \left(\frac{\nkz\ykz + \sum_{i=1}^{e_k} (t_i^k)^\kappa + (n_k-e_k) (\tnow)^\kappa }%
               {\nkz\ykz + \sum_{i=1}^{e_k} (t_i^k)^\kappa + (n_k-e_k-l_k-j) (\tnow)^\kappa + (l_k + j) t^\kappa }\right)^{%
    \nkz + e_k + 1} 
\end{align}
\end{linenomath*}

While it is clear that the lower bounds for $\ykz$ will lead to the lower system survival function,
the role of $\nkz$ is not so clear.
We therefore resort to numerical optimization of \eqref{eq:25} over $\{\nkz, k=1,\ldots,K\}$
to obtain the lower bound for $P(\Tsys > t\mid\{\nkz,\ykz, \mbf{t}^k_{e_k;n_k}\}_{k=1}^K)$.


\section{elicitation}

*** prior via pseudo-observations, can contain censored pseudo-data 

*** cf paper from Frank \cite{1996:coolen::cens}


\pagebreak
%
%
% Now we start the appendices, with the new section name, "Appendix", and a 
%  new counter, "I", "II", etc.
%
\appendix\label{section:references}
%
% Here's the first appendix, the list of references:
%
\bibliography{refs}
%
%\cite{key}   produces citations with full author list and year 
%\citeNP{key} produces citations with full author list and year, but without enclosing parentheses
%\citeA{key}  produces citations with only the full author list
%\citeN{key}  produces citations with the full author list and year, but
%             which can be used as nouns in a sentence; no parentheses appear around
%             the author names, but only around the year
%\citeyear{key}   produces the year information only, within parentheses
%\citeyearNP{key} produces the year information only
%
%
% And now for some pretty impressive notation.  In this example, I have used
%   the tabular environment to line up the columns in ASCE style.
%   Note that this and all appendices (except the references) start with 
%   the \section command
%
\section{Notation}
\emph{The following symbols are used in this paper:}%\par\vspace{0.10in}
\nopagebreak
\par
\begin{tabular}{r  @{\hspace{1em}=\hspace{1em}}  l}
$D$                    & pile diameter (m); \\
$R$                    & distance (m);      and\\
$C_{\mathrm{Oh\;no!}}$ & fudge factor.
\end{tabular}


\end{document}
